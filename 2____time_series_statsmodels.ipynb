{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa28a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### statsmodels/regression/TimeSeries ######\n",
    "## Notes: The code already includes as many models as possible, \n",
    "# but specific adjustments will need to be made based on the project.\n",
    "\n",
    "# Also, below steps are not strictly in such an order.\n",
    "\n",
    "# 0. Import necessary libraries.\n",
    "# 1. Visualize the dataset according to its characteristics\n",
    "# 2. Load and preprocess the dataset \n",
    "# 3. Define the predictors (X) and the target variable (y).\n",
    "# 4. Split the dataset into training and testing sets.\n",
    "# 5. Define a function to fit different regression models, \n",
    "    # evaluate them using metrics such as MSE, RMSE, and MAE, and return the best model.\n",
    "###### statsmodels/regression/TimeSeries ######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa5dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Import Libraries\n",
    "import warnings # for muting warning messages\n",
    "# mute warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "# Single series models\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.dynamic_factor import DynamicFactor\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Multivariate models\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from ISLP import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e8707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. EDA \n",
    "\n",
    "# 1.1 Time Series Plot\n",
    "data, time_series = load_dataset('household_power_consumption.txt')\n",
    "data_processed = data_preprocess(data)\n",
    "data_processed.set_index('datetime', inplace=True)\n",
    "\n",
    "# # Handle missing values (if any)\n",
    "# data = data.interpolate(method='time')\n",
    "data_subset=data_processed['2009-12-01':'2010-01-01']\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(data_subset)\n",
    "plt.title('Household Power Consumption')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 1.2 Time Series Plot with Trend, Seasonal and  Residual\n",
    "data, time_series = load_dataset('household_power_consumption.txt')\n",
    "data_processed = data_preprocess(data)\n",
    "data_processed.set_index('datetime', inplace=True)\n",
    "data_subset=data_processed['2010-01-01':'2010-01-15']\n",
    "# Example: Decompose 'Global_active_power' assuming a seasonal pattern every 24 hours (1440 minutes)\n",
    "decompfreq = 60  # Assuming daily seasonality (24 hours)\n",
    "decomposition = sm.tsa.seasonal_decompose(data_subset['Global_active_power'], period= decompfreq)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plotting the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10))\n",
    "\n",
    "# Original time series\n",
    "ax1.plot(data_subset.index, data_subset['Global_active_power'], label='Original')\n",
    "ax1.set_title('Original Time Series')\n",
    "ax1.legend()\n",
    "\n",
    "# Trend component\n",
    "ax2.plot(data_subset.index, trend, label='Trend', color='orange')\n",
    "ax2.set_title('Trend Component')\n",
    "ax2.legend()\n",
    "\n",
    "# Seasonal component\n",
    "ax3.plot(data_subset.index, seasonal, label='Seasonal', color='green')\n",
    "ax3.set_title('Seasonal Component')\n",
    "ax3.legend()\n",
    "\n",
    "# Residual component\n",
    "ax4.plot(data_subset.index, residual, label='Residual', color='red')\n",
    "ax4.set_title('Residual Component')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#1.3 Time Series Plot with Aggregation monthly data and normalized it\n",
    "data, time_series = load_dataset('household_power_consumption.txt')\n",
    "data_processed = data_preprocess(data)\n",
    "data_processed.set_index('datetime', inplace=True)\n",
    "\n",
    "numerical_columns = data_processed.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate mean for each year (excluding 'datetime')\n",
    "# Group by year and month, and calculate mean\n",
    "data_mean_monthly = numerical_columns.groupby([numerical_columns.index.year, numerical_columns.index.month]).mean()\n",
    "data_mean_monthly.reset_index(drop=True, inplace=True)  # Reset index to numeric indices\n",
    "# Min-Max normalization\n",
    "data_mean_monthly_normalized = (data_mean_monthly - data_mean_monthly.min()) / (data_mean_monthly.max() - data_mean_monthly.min())\n",
    "print(data_mean_monthly_normalized)\n",
    "\n",
    "# 1.4 Time Series Plot with Aggregation monthly data, Trend, Seasonal, and Residual\n",
    "data, time_series = load_dataset('household_power_consumption.txt')\n",
    "data_processed = data_preprocess(data)\n",
    "data_processed.set_index('datetime', inplace=True)\n",
    "\n",
    "numerical_columns = data_processed.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate mean for each year (excluding 'datetime')\n",
    "# Group by year and month, and calculate mean\n",
    "data_mean_monthly = numerical_columns.groupby([numerical_columns.index.year, numerical_columns.index.month]).mean()\n",
    "data_mean_monthly.reset_index(drop=True, inplace=True)  # Reset index to numeric indices\n",
    "# Min-Max normalization\n",
    "data_mean_monthly_normalized = (data_mean_monthly - data_mean_monthly.min()) / (data_mean_monthly.max() - data_mean_monthly.min())\n",
    "\n",
    "\n",
    "# Perform seasonal decomposition for each monthly period\n",
    "decompfreq = 12  # Assuming annual seasonality (12 months)\n",
    "decomposition = sm.tsa.seasonal_decompose(data_mean_monthly_normalized['Global_active_power'], period= decompfreq)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plotting the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10))\n",
    "\n",
    "# Original time series\n",
    "ax1.plot(data_mean_monthly_normalized.index, data_mean_monthly_normalized['Global_active_power'], label='Original')\n",
    "ax1.set_title('Original Time Series')\n",
    "ax1.legend()\n",
    "\n",
    "# Trend component\n",
    "ax2.plot(data_mean_monthly_normalized.index, trend, label='Trend', color='orange')\n",
    "ax2.set_title('Trend Component')\n",
    "ax2.legend()\n",
    "\n",
    "# Seasonal component\n",
    "ax3.plot(data_mean_monthly_normalized.index, seasonal, label='Seasonal', color='green')\n",
    "ax3.set_title('Seasonal Component')\n",
    "ax3.legend()\n",
    "\n",
    "# Residual component\n",
    "ax4.plot(data_mean_monthly_normalized.index, residual, label='Residual', color='red')\n",
    "ax4.set_title('Residual Component')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 1.5 Time Series Plot with Aggregation monthly data and normalized it, with all variables\n",
    "data, time_series = load_dataset('household_power_consumption.txt')\n",
    "data_processed = data_preprocess(data)\n",
    "data_processed.set_index('datetime', inplace=True)\n",
    "\n",
    "numerical_columns = data_processed.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate mean for each year (excluding 'datetime')\n",
    "# Group by year and month, and calculate mean\n",
    "data_mean_monthly = numerical_columns.groupby([numerical_columns.index.year, numerical_columns.index.month]).mean()\n",
    "data_mean_monthly.reset_index(drop=True, inplace=True)  # Reset index to numeric indices\n",
    "# Min-Max normalization\n",
    "data_mean_monthly_normalized = (data_mean_monthly - data_mean_monthly.min()) / (data_mean_monthly.max() - data_mean_monthly.min())\n",
    "\n",
    "print(len(data_mean_monthly))\n",
    "plt.figure(figsize=(14, 8))\n",
    "# Plot each column\n",
    "for col in data_mean_monthly_normalized.columns:\n",
    "    # if '_active' in col or 'intensity' in col or 'Voltage' in col:  # Skip the datetime column\n",
    "    plt.plot(data_mean_monthly_normalized.index, data_mean_monthly_normalized[col], marker='o', label=col)\n",
    "\n",
    "plt.title('Yearly Means of Electrical Power Measurements')\n",
    "plt.xlabel('Year and Month')\n",
    "plt.ylabel('Measurement Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bb657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Load the dataset\n",
    "def find_filepath(name):\n",
    "    for root, dirs, files in os.walk('..'):\n",
    "        for file in files:\n",
    "            base, extension = os.path.splitext(file)\n",
    "            if extension.lower() in ('.txt', '.csv'):\n",
    "                if os.path.splitext(name)[1]:  # If name has an extension\n",
    "                    if file.lower() == name.lower():  # Compare filename with name\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        if os.path.isfile(file_path):\n",
    "                            return file_path\n",
    "                else:  # If name does not have an extension\n",
    "                    if base == os.path.splitext(name)[0]:  # Compare base part of filename with name\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        if os.path.isfile(file_path):\n",
    "                            return file_path\n",
    "    return None\n",
    "\n",
    "def get_dataframe(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "def capitalize_first_letter(word):\n",
    "    # Capitalize the first letter and convert the rest of the word to lowercase\n",
    "    return word[:1].upper() + word[1:].lower()\n",
    "\n",
    "def calculate_dynamic_blocksize(file_path,partitions):\n",
    "    # Get file size in bytes\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    \n",
    "    # Calculate blocksize in bytes\n",
    "    block_size = file_size // partitions\n",
    "    \n",
    "    # Convert blocksize to MB\n",
    "    block_size_mb = block_size / (1024 * 1024)\n",
    "    \n",
    "    return f\"{block_size_mb}MB\"\n",
    "\n",
    "def load_dataset(name, partitions=10, sept =';', infer_datetime = True, low_mem=False):\n",
    "    time_series=False\n",
    "    try: \n",
    "        capitalized_name = capitalize_first_letter(name)\n",
    "\n",
    "        data = load_data(f\"{capitalized_name}\")\n",
    "        \n",
    "        return get_dataframe(data), time_series\n",
    "    except:\n",
    "        file_path=find_filepath(name)\n",
    "        if file_path:\n",
    "            blocksize = calculate_dynamic_blocksize(file_path, partitions)\n",
    "            data = dd.read_csv(\n",
    "                    file_path, \n",
    "                    sep=sept, \n",
    "                    infer_datetime_format=infer_datetime,  \n",
    "                    na_values='?', \n",
    "                    low_memory=low_mem, # load all data into memory at one time to facilitate to infer the data types\n",
    "                    blocksize = blocksize   \n",
    "            )\n",
    "            original_column_list = data.columns.tolist()\n",
    "            # Convert all elements to lowercase\n",
    "            lowercase_column_list = [element.lower() for element in original_column_list]\n",
    "            if 'date' in lowercase_column_list and 'time' in lowercase_column_list:\n",
    "                for col in original_column_list:\n",
    "                    if col.lower() == 'date':\n",
    "                        data = data.rename(columns={col: 'Date'})\n",
    "                    elif col.lower() == 'time':\n",
    "                        data = data.rename(columns={col: 'Time'})\n",
    "                data['datetime']= dd.to_datetime(data['Date'] + ' ' + data['Time'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "                # Optional: Handle any rows where datetime parsing failed by filling with NaN\n",
    "                data['datetime'].fillna(pd.NaT)\n",
    "               \n",
    "                time_series =True\n",
    "                data = data.drop(columns = ['Date','Time'])\n",
    "\n",
    "            # Persist the dataframe to speed up future operations\n",
    "            data = data.persist()\n",
    "            \n",
    "            return data, time_series\n",
    "        else: \n",
    "            return 'No dataset named ' + capitalized_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694327ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess the dataset\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoder(X, cat_feature):\n",
    "    freq = X[cat_feature].value_counts(normalize=True)\n",
    "    return X[cat_feature].map(freq).fillna(0)\n",
    "\n",
    "def get_num_cat_columns(dataframe):\n",
    "    # Define numeric, categorical, and datetime features\n",
    "    numeric_features = []\n",
    "    categorical_features = []\n",
    "    datetime_features = []\n",
    "\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype in ['int64', 'float64']:\n",
    "            # Exclude binary columns\n",
    "            unique_values = dataframe[col].dropna().unique()\n",
    "            if len(unique_values) > 2:  # More than 2 unique values (not binary)\n",
    "                numeric_features.append(col)\n",
    "            else:  # Binary columns\n",
    "                categorical_features.append(col)\n",
    "        elif dataframe[col].dtype == 'object' or dataframe[col].dtype.name == 'category':\n",
    "            categorical_features.append(col)\n",
    "        elif dataframe[col].dtype == 'datetime64[ns]':\n",
    "            datetime_features.append(col)\n",
    "\n",
    "    print(\"Numeric Features:\", numeric_features)\n",
    "    print(\"Categorical Features:\", categorical_features)\n",
    "    print(\"Datetime Features:\", datetime_features)\n",
    "    return numeric_features, categorical_features, datetime_features\n",
    "\n",
    "def data_preprocess(data, scale =False):\n",
    "    # dataframe = get_dataframe(data)\n",
    "    dataframe = data\n",
    "  \n",
    "    # 0. Create empty DataFrames for transformed numeric and categorical data\n",
    "    X_num_transformed = dd.DataFrame()\n",
    "    X_cat_transformed = dd.DataFrame()\n",
    "\n",
    "    # Define numericï¼Œ categorical and datetime features\n",
    "    numeric_features, categorical_features, datetime_features = get_num_cat_columns(dataframe.compute().head(100))\n",
    "\n",
    "    # 1. Process numeric features\n",
    "    if len(numeric_features) > 0:\n",
    "       \n",
    "        # Define numeric transformer \n",
    "        imputer_num = SimpleImputer(strategy='constant', fill_value=0)\n",
    "        X_num_imputed = imputer_num.fit_transform(dataframe[numeric_features])\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_num_transformed = scaler.fit_transform(X_num_imputed)\n",
    "    else:\n",
    "        X_num_transformed = X_num_imputed\n",
    "\n",
    "    # 2. Process datetime features\n",
    "    if len(datetime_features) > 0:\n",
    "        # Interpolate missing values in datetime columns\n",
    "        for datetime_feature in datetime_features:\n",
    "            dataframe[datetime_feature] = dataframe[datetime_feature].interpolate(method='linear')\n",
    "\n",
    "    # 3. Define categorical transformer steps\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    # Process categorical features\n",
    "    if len(categorical_features) > 0:\n",
    "        print(f'Categorical features found. {categorical_features}')\n",
    "        X_cat = dataframe[categorical_features]\n",
    "        X_cat = dd.utils.make_categorical(X_cat, columns=categorical_features)\n",
    "\n",
    "        for cat_feature in categorical_features:\n",
    "            X_cat_transformed[cat_feature] = frequency_encoder(X_cat.compute(), cat_feature)\n",
    "\n",
    "    # Combine processed numeric, datetime and categorical features\n",
    "    X_transformed = dd.concat([X_num_transformed, X_cat_transformed, dataframe[datetime_features] ], axis=1)\n",
    "\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Pipeline fitting\n",
    "# 4.1 Subfunctions for calculating metrics\n",
    "def compute_forecast_interval_coverage(y_actual, lower_bounds, upper_bounds):\n",
    "    assert len(y_actual) == len(lower_bounds) == len(upper_bounds), \"Lengths of actual and intervals should match\"\n",
    "    \n",
    "    num_forecasts = len(y_actual)\n",
    "    num_covered = 0\n",
    "    \n",
    "    for i in range(num_forecasts):\n",
    "        if lower_bounds[i] <= y_actual[i] <= upper_bounds[i]:\n",
    "            num_covered += 1\n",
    "    \n",
    "    coverage = num_covered / num_forecasts\n",
    "    \n",
    "    return coverage\n",
    "\n",
    "def calculate_metrics(result, y_pred, y_test, model_name_with_degree, forecast_intervals=None):\n",
    "    # Define the calculations and their corresponding functions\n",
    "    model_metrics={}\n",
    "    model_metrics[\"name\"] = model_name_with_degree\n",
    "    # model_metrics[\"model\"] = model_class\n",
    "    # model_metrics[\"degree\"] = degree\n",
    "    calculations = {\n",
    "        \"aic\": lambda result: result.aic,\n",
    "        \"bic\": lambda result: result.bic,\n",
    "        \"likelihood\": lambda result: result.llf,\n",
    "        \"rmse\": lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": mean_absolute_error,\n",
    "        \"mape\": lambda y_true, y_pred: np.mean(np.abs((y_true - y_pred) / y_true)) * 100,     \n",
    "        \"forecast_interval_coverage\": lambda y_true, intervals: compute_forecast_interval_coverage(y_true, intervals[:, 0], intervals[:, 1]),\n",
    "        \"forecast_bias\": lambda y_true, y_pred: np.mean(y_pred - y_true)\n",
    "    }\n",
    "    \n",
    "    # Perform each calculation and handle errors\n",
    "    for metric_name, func in calculations.items():\n",
    "        try:\n",
    "            if metric_name in (\"aic\", \"bic\", \"likelihood\"):\n",
    "                if hasattr(result, metric_name):\n",
    "                   value = func(result)\n",
    "                else: \n",
    "                    value = np.inf if metric_name in (\"aic\", \"bic\") else -np.inf\n",
    "\n",
    "            elif metric_name == \"forecast_interval_coverage\":\n",
    "                # Only calculate the coverage when intervals are provided\n",
    "                if forecast_intervals:\n",
    "                    value = func(y_test, forecast_intervals)\n",
    "                else:\n",
    "                    value = - np.inf\n",
    "            else:\n",
    "                value = func(y_test, y_pred)\n",
    "        \n",
    "            # print(f\"{name} : {value}\")\n",
    "            model_metrics[f\"{metric_name}\"] = value\n",
    "            \n",
    "        except Exception as e:\n",
    "            if metric_name in (\"forecast_interval_coverage\", \"likelihood\"):\n",
    "                model_metrics[f\"{metric_name}\"] = - np.inf\n",
    "            else: \n",
    "                model_metrics[f\"{metric_name}\"] = np.inf\n",
    "\n",
    "    model_metrics[\"result\"] = result \n",
    "    return model_metrics\n",
    "\n",
    "def aggregate_metrics_list(metrics_list):\n",
    "    # Aggregate metrics\n",
    "    df_metrics_list = get_dataframe(metrics_list)\n",
    "\n",
    "    # Group by 'name' and aggregate metrics\n",
    "    grouped = df_metrics_list.groupby('name')\n",
    "    aggregated_metrics = grouped.agg(lambda x: np.mean(x) if np.issubdtype(x.dtype, np.number) else x.iloc[0])\n",
    "    \n",
    "    # return aggregated_metrics.to_dict(orient='index')\n",
    "    return aggregated_metrics\n",
    "\n",
    "def sort_metrics_list(df_metrics_aggregated):\n",
    "    # Sort the metrics by priority \n",
    "    df_metrics=get_dataframe(df_metrics_aggregated)\n",
    "    # Helper function to sort values, treating strings as high values\n",
    "    def custom_sort(val, ascending=True):\n",
    "        try:\n",
    "            return (float(val),) if ascending else (-float(val),)\n",
    "        except ValueError:\n",
    "            return (float('-inf'),) if ascending else (float('inf'),)\n",
    "    # Columns and their sort order\n",
    "    sort_columns = ['aic', 'bic', 'likelihood', 'rmse', 'mae', 'mape', 'forecast_interval_coverage', 'forecast_bias']\n",
    "    ascending_order = [True, True, False, True, True, True, False, True]   \n",
    "    df_metrics = df_metrics.sort_values(by=sort_columns, ascending=ascending_order)\n",
    "    # Apply custom sorting\n",
    "    for col, asc in zip(sort_columns, ascending_order):\n",
    "        df_metrics[col + '_sort'] = df_metrics[col].apply(lambda x: custom_sort(x, ascending=asc))\n",
    "\n",
    "    # Perform the sorting based on the new sort columns\n",
    "    sort_by = [col + '_sort' for col in sort_columns]\n",
    "    df_metrics = df_metrics.sort_values(by=sort_by)\n",
    "    # Drop the sort helper columns\n",
    "    df_metrics = df_metrics.drop(columns=sort_by)\n",
    "    return df_metrics  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Subfunctions for poly nomial features\n",
    "def poly_transformer(train_data, test_data, degree = 1, scale=False):\n",
    "    \n",
    "    column_names=train_data.columns.tolist() \n",
    "    poly = PolynomialFeatures(degree = degree, include_bias=False)\n",
    "    train_poly = poly.fit_transform(train_data)\n",
    "    test_poly = poly.transform(test_data)\n",
    "    feature_names = poly.get_feature_names_out(column_names)\n",
    "    # Convert to DataFrame to retain column names\n",
    "    train_poly_df = pd.DataFrame(train_poly, columns=feature_names, index=train_data.index)\n",
    "    test_poly_df = pd.DataFrame(test_poly, columns=feature_names, index=test_data.index)\n",
    "    \n",
    "    if scale:\n",
    "        train_poly_scaled =data_preprocess(train_poly,True)\n",
    "        test_poly_scaled =data_preprocess(test_poly, True)\n",
    "        return train_poly_scaled, test_poly_scaled, feature_names\n",
    "    return train_poly_df, test_poly_df, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 subfuctions for prediction intervals\n",
    "def get_confidence_intervals(X_train_poly, X_test_poly, y_train, result, num_bootstrap_samples=50,confidence = 0.95):\n",
    "    bootstrap_predictions =[]\n",
    "    prediction_intervals = []\n",
    "    \n",
    "    for _ in range(num_bootstrap_samples):\n",
    "        # Generate bootstrapped indices within the specified range\n",
    "        indices = np.random.choice(np.arange(0, len(X_train_poly)), len(X_train_poly), replace=True)\n",
    " \n",
    "        # Use .iloc[indices] with dataframe, and [indices] with np.arrays\n",
    "        # Note: after fittransform, a np.arrays is returned\n",
    "        # Get one set of bootstrap sample data\n",
    "        X_bootstrap= X_train_poly[indices]\n",
    "        y_bootstrap= y_train.iloc[indices]\n",
    "       \n",
    "        # Use the same model type of best_model\n",
    "        bootstrap_model=result.model.__class__\n",
    "        family = result.model.family if hasattr(result.model, 'family') else None\n",
    "        if family: \n",
    "            bootstrap_result = bootstrap_model(y_bootstrap, sm.add_constant(X_bootstrap), family=family).fit()\n",
    "        else:\n",
    "            bootstrap_result = bootstrap_model(y_bootstrap, sm.add_constant(X_bootstrap)).fit()\n",
    "\n",
    "        # Make predictions on the test data using the new result\n",
    "        predictions_sample = bootstrap_result.predict(sm.add_constant(X_test_poly))\n",
    "        # Append predictions to the list\n",
    "        bootstrap_predictions.append(predictions_sample)\n",
    "\n",
    "    # Compute prediction intervals \n",
    "    lower_bound = np.percentile(bootstrap_predictions, (1 - confidence) * 100 / 2, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_predictions, (1 + confidence) * 100 / 2, axis=0)\n",
    "    prediction_intervals = np.column_stack((lower_bound, upper_bound))\n",
    "    return prediction_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f310b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Pipeline fitting with grid search and hyperparams tuning\n",
    "def fit_time_series_models(logger, univariate, multivariate, testsize_split,\n",
    "                          degree_list=None, num_bootstrap_samples=50,\n",
    "                          confidence=0.95):\n",
    "    # 5. Fit the models\n",
    "    # 5.1 Define the models to fit\n",
    "    \n",
    "    models = {\n",
    "        \"ARIMA\": {\n",
    "            \"class\": SARIMAX,\n",
    "            \"poly_supported\": False,\n",
    "            \"endog\": \"univariate\",\n",
    "            \"params\": [\n",
    "                {\"order\": (2, 0, 3)}\n",
    "            ]\n",
    "        },\n",
    "        \"SARIMA\": {\n",
    "            \"class\": SARIMAX,\n",
    "            \"poly_supported\": False,\n",
    "            \"endog\": \"univariate\",\n",
    "            \"params\": [\n",
    "                {\"order\": (2, 0, 3), \"seasonal_order\": (0, 0, 0, 1440), \"exog\": None}\n",
    "            ]\n",
    "        },\n",
    "        \"VAR\": {\n",
    "            \"class\": VAR,\n",
    "            \"poly_supported\": True,\n",
    "            \"endog\": \"multivariate\",\n",
    "            \"params\": [\n",
    "                {\"maxlags\": 7, \"freq\": 'T', \"trend\": 'c'}\n",
    "            ]\n",
    "        },\n",
    "        \"VARMAX\": {\n",
    "            \"class\": VARMAX,\n",
    "            \"poly_supported\": True,\n",
    "            \"endog\": \"multivariate\",\n",
    "            \"params\": [\n",
    "                {\"order\": (2, 2), \"exog\": None}\n",
    "            ]\n",
    "        },\n",
    "        \"ExponentialSmoothing\": {\n",
    "            \"class\": ExponentialSmoothing,\n",
    "            \"poly_supported\": False,\n",
    "            \"endog\": \"univariate\",\n",
    "            \"params\": [\n",
    "                {\"trend\": \"add\", \"seasonal\": \"add\", \"seasonal_periods\": 1440, \"exog\": None}\n",
    "            ]\n",
    "        },\n",
    "        \"AutoReg\": {\n",
    "            \"class\": AutoReg,\n",
    "            \"poly_supported\": True,\n",
    "            \"endog\": \"univariate\",\n",
    "            \"params\": [\n",
    "                {\"lags\": 7, \"trend\": 'c', \"method\": 'cmle'}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    models = {\n",
    "        \"ARIMA\": {\n",
    "            \"class\": SARIMAX,\n",
    "            \"poly_supported\": False,\n",
    "            \"endog\": \"univariate\",\n",
    "            \"params\": [\n",
    "                {\"order\": (2, 0, 3)}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    # Default to linear (degree 1) if degree_list is None or empty\n",
    "    if degree_list is None:\n",
    "        degree_list = [1]\n",
    "    else:\n",
    "        if not isinstance(degree_list, list):\n",
    "            print('Degree list is not a list.')\n",
    "            degree_list = [degree_list]  # Ensure degree_list is converted to a list if it's not already\n",
    "        if 1 not in degree_list:\n",
    "            degree_list.append(1)  \n",
    "\n",
    "    # 5.2 TimeSeries Split\n",
    "    tscv = TimeSeriesSplit(n_splits=testsize_split)  # Define the number of splits\n",
    "\n",
    "    best_model = None\n",
    "    metrics_list = []\n",
    "    for model_name, model_info in models.items():\n",
    "        model_class = model_info[\"class\"]\n",
    "        poly_supported = model_info[\"poly_supported\"]\n",
    "        endog_type = model_info[\"endog\"]\n",
    "        if endog_type == 'univariate':\n",
    "            endog = univariate\n",
    "        else: \n",
    "            endog = multivariate\n",
    "\n",
    "        for degree in degree_list:\n",
    "            if not poly_supported:\n",
    "                if degree != 1:\n",
    "                    continue  # Skip degrees other than 1 for models that do not support polynomial features\n",
    "        \n",
    "            # Cross Validation\n",
    "            for train_index, test_index in tscv.split(endog):\n",
    "                train_data, test_data = endog.iloc[train_index], endog.iloc[test_index]\n",
    "                print(f\"Train data contains {len(train_data)} rows.\")\n",
    "                print(f\"Test data contains {len(test_data)} rows.\")\n",
    "                \n",
    "                # 5.3 Transform the dataset\n",
    "                # Only add poly features to those models supporting poly features\n",
    "                train_poly, test_poly, _ = poly_transformer(train_data, test_data, degree)\n",
    "    \n",
    "                # 5.4 Find the best params and fit the model\n",
    "                model_name_with_degree = f\"{model_name} Degree {degree}\" \n",
    "                try:\n",
    "                    # Print start training message\n",
    "                    # print(f\"Start training {model_name_with_degree}...\")\n",
    "                    start_time = time.time()\n",
    "                    model_params = model_info['params']\n",
    "                    params_str = \", \".join([f\"{key}={value}\" for key, value in model_params.items()])\n",
    "                    fitted_model = model_class(train_poly, params_str).fit()\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    execution_time = end_time - start_time\n",
    "                    minutes = int(execution_time // 60)\n",
    "                    seconds = int(execution_time % 60)\n",
    "                    print(\"Training Complete. \", model_name_with_degree, \" Execution time:\", minutes, \"minutes and\", seconds, \"seconds\")\n",
    "                                        \n",
    "                    # Make predictions\n",
    "                    predictions = fitted_model.predict(start=len(train_poly), end=len(train_poly) + len(test_poly) - 1)\n",
    "        \n",
    "                    # Calculate metrics\n",
    "                    model_metrics = calculate_metrics(fitted_model, predictions, test_data, model_name_with_degree)\n",
    "                    \n",
    "                    # Store metrics\n",
    "                    metrics_list.append(model_metrics)    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting {model_name_with_degree} model:\", str(e))\n",
    "                    continue\n",
    "            \n",
    "    # 5.6 Choose the best model\n",
    "    # 5.6.1 Aggregate the metrics for the same model_name_with_degree\n",
    "    df_metrics_aggregated = aggregate_metrics_list(metrics_list)    \n",
    "    # 5.6.2 Sort the metrics \n",
    "    df_metrics = sort_metrics_list(df_metrics_aggregated)\n",
    "    print(df_metrics)\n",
    "    # save_file(df_metrics, 'metrics_boston', 'Boston Metrics - sm') \n",
    "\n",
    "    best_model_info=df_metrics.iloc[0]\n",
    "    best_model_name =best_model_info['name'] \n",
    "    best_model = best_model_info['result'] # result is derived instead of model name\n",
    "    degree = best_model_info['degree']\n",
    "    print(f'Best model found: {best_model_name}, Degree: {degree}')\n",
    "    # print(best_model_info)\n",
    "   \n",
    "    X_train_poly, X_test_poly, feature_names = poly_transformer(X_train, X_test, degree)\n",
    "\n",
    "    # 5.7 (Optional) Calculate the prediction_intervals based on the best model\n",
    "    # with train data using boostrap resampling\n",
    "    prediction_intervals = get_confidence_intervals(X_train_poly, X_test_poly, y_train, best_model, num_bootstrap_samples,confidence)\n",
    "\n",
    "    return best_model, degree, top_features,exog_feature_mapping, prediction_intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # 0. Configure the logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,                 # Set log level to INFO, which logs INFO and higher levels\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "        filename='sm_time_series.log',                 # Log to file named app.log\n",
    "        filemode='w'                        # Write mode (overwrite existing log)\n",
    "    )\n",
    "\n",
    "    # Create a logger instance\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('This is an INFO message')\n",
    "    logger.warning('This is a WARNING message')\n",
    "    logger.error('This is an ERROR message')\n",
    "\n",
    "    # 2. Load dataset\n",
    "    data, time_series = load_dataset('household_power_consumption.txt')\n",
    "\n",
    "    \n",
    "    # Specify the target variable(s)\n",
    "    uni_target_variable = 'Global_active_power'\n",
    "    multi_target_variable = ['Global_active_power', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "    data_subset = data[multi_target_variable].head(100)\n",
    "\n",
    "    # 3. Preprocess dataset\n",
    "    data_processed = data_preprocess(data_subset)\n",
    "    # Setting the datetime column as the index \n",
    "    # in a time series dataset can be beneficial and is often recommended.\n",
    "    data_processed.set_index('datetime', inplace=True)\n",
    "\n",
    "    # 4. Define endog (optianal exog)\n",
    "    \n",
    "    # Get the data for modeling\n",
    "    univariate = data_processed[uni_target_variable]\n",
    "    multivariate = data_processed[multi_target_variable]\n",
    "\n",
    "    # 5. Fit regression models and get the optimal model\n",
    "    # 5.1 Specify the degree of the polynomial, test size, cross validation and others\n",
    "    degree_list = [1]  \n",
    "    testsize_split = 5  \n",
    "    confidence=0.95\n",
    "    num_bootstrap_samples=50\n",
    "\n",
    "    # 5.2 Fit the time series model using Loop\n",
    "    fit_time_series_models(logger, univariate, multivariate, testsize_split,\n",
    "                          degree_list, num_bootstrap_samples,\n",
    "                          confidence)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "###### statsmodels/time series ######"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "497a84dc8fec8cf8d24e7e87b6d954c9a18a327edc66feb9b9ea7e9e72cc5c7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
