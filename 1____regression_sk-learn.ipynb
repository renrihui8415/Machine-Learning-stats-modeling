{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn VS. Statsmodels \n",
    "\n",
    "- **scikit-learn library**: \n",
    "    - It is primarily used for machine learning and predictive modeling. \n",
    "    - It provides a simple and efficient tool for data mining and data analysis.\n",
    "\n",
    "- Input: For example, the model is initialized with LinearRegression(), and the data is fitted with lm.fit().\n",
    "- Output: After fitting, the main outputs are the coefficients (lm.coef_) and intercept (lm.intercept_). \n",
    "\n",
    "- It provides tools for making predictions (lm.predict()) but less detailed statistical summaries.\n",
    "\n",
    "\n",
    "- **statsmodels**: \n",
    "    - It focused on statistical modeling. \n",
    "    - It provides more detailed output related to the statistical properties of the model, making it useful for inferential statistics.\n",
    "- Input: For example, the model is initialized with sm.OLS(y, X), and the data is fitted with model.fit().\n",
    "- Output: The results object provides a comprehensive summary (results.summary()) including coefficients, standard errors, t-values, p-values, R^2, adjusted R^2, F-statistic, and more. \n",
    "\n",
    "- It’s designed to give a full statistical report.\n",
    "\n",
    "We can integrate both into your workflow: using scikit-learn for prediction and statsmodels for detailed statistical analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classification Models VS. Regression Models**\n",
    "\n",
    "`Classification Models`\n",
    "\n",
    "**Nature of Output:**\n",
    "\n",
    "- Discrete Labels: Classification models predict categorical outcomes. For example, determining whether an email is spam or not spam (binary classification) or classifying an image into one of several categories (multiclass classification).\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Class Membership: The primary goal is to assign input data to one of several predefined categories or classes.\n",
    "\n",
    "**Common Algorithms:**\n",
    "\n",
    "- Logistic Regression: Used for binary classification problems.\n",
    "- Decision Trees and Random Forests: Can be used for both binary and multiclass classification.\n",
    "- Support Vector Machines (SVM): Effective for high-dimensional spaces.\n",
    "- K-Nearest Neighbors (KNN): A simple, instance-based learning algorithm.\n",
    "- Gradient Boosting Machines (GBM): An ensemble technique that builds models sequentially.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "- Accuracy: The proportion of correctly predicted instances.\n",
    "- Precision, Recall, F1-Score: Used especially when dealing with imbalanced datasets.\n",
    "- ROC-AUC: Measures the ability of the model to distinguish between classes.\n",
    "\n",
    "`Regression Models`\n",
    "\n",
    "**Nature of Output:**\n",
    "\n",
    "- Continuous Values: Regression models predict continuous numeric outcomes. For example, predicting house prices or the temperature for a given day.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Value Estimation: The primary goal is to estimate the value of the output variable based on the input features.\n",
    "\n",
    "**Common Algorithms:**\n",
    "\n",
    "- Linear Regression: A fundamental and widely used method for predicting continuous values.\n",
    "- Ridge and Lasso Regression: Extensions of linear regression that include regularization.\n",
    "- Decision Trees and Random Forests: Can also be used for regression tasks.\n",
    "- Support Vector Regression (SVR): An extension of SVM for regression problems.\n",
    "- Gradient Boosting Regressors: Similar to classification, but tailored for continuous outcomes.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "- Mean Squared Error (MSE): The average of the squared differences between the predicted and actual values.\n",
    "- Root Mean Squared Error (RMSE): The square root of the MSE, providing error in the same units as the output variable.\n",
    "- Mean Absolute Error (MAE): The average of the absolute differences between the predicted and actual values.\n",
    "- R-squared: Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### scikit-learn/regression ######\n",
    "# 0. Import Required Libraries\n",
    "# 1. Visualize dataset according to data characteristics\n",
    "# 2. Load the Dataset (Boston dataset in this case)\n",
    "# 3. Split the Data\n",
    "# 4. Define Regression Pipelines with hyperparams\n",
    "# 5. Train Pipelines with grid search\n",
    "# 6. Return the Best Model and Visualize Performance Metrics\n",
    "# 7. Predicting with the best model\n",
    "\n",
    "# Please note: the above steps are not strictly in such an order, \n",
    "# steps may overlap each other.\n",
    "\n",
    "# The code already includes as many models as possible, \n",
    "# but specific adjustments will need to be made based on the project.\n",
    "###### scikit-learn/regression ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Import Required Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import validation_curve\n",
    "from ISLP import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings # for muting warning messages\n",
    "# mute warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize dataset according to data characteristics\n",
    "# 1.1 Subfunctions to plot for dataset\n",
    "def plot_all_data(df,df_without_target, target_variable,model, feature_names):\n",
    "\n",
    "    # visualize_distribution(df_without_target)\n",
    "    visualize_correlation_heatmap(df)\n",
    "    # visualize_scatter_matrix(df, target_variable)\n",
    "    # Identify categorical features\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "    print(\"Categorical features:\", categorical_features)\n",
    "    \n",
    "    if len(categorical_features) > 0:\n",
    "        for categorical_feature in categorical_features:\n",
    "            visualize_boxplots(df, target_variable, categorical_feature)\n",
    "    else:\n",
    "        print(\"No categorical features found for boxplots.\")\n",
    "\n",
    "    visualize_pairwise_scatter_plots(df)\n",
    "    visualize_feature_importance(model, feature_names)\n",
    "    \n",
    "# Histograms and Density Plots\n",
    "# Each histogram represents the distribution of values within a specific feature.\n",
    "def visualize_distribution(df):\n",
    "    df.hist(bins=20, figsize=(15, 10))\n",
    "    plt.suptitle(\"Histograms of Features\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "# It shows one feature increases, the other features increases or decreases\n",
    "def visualize_correlation_heatmap(df):\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "# Scatter Plot Matrix\n",
    "# The pair plot shows scatter plots for each pair of features in the DataFrame. \n",
    "# Each subplot represents the relationship between two features.\n",
    "def visualize_scatter_matrix(df, target_variable):\n",
    "    sns.pairplot(df.sample(frac=0.1, random_state=42), hue=target_variable, palette='bwr')\n",
    "    plt.title(\"Pairplot of Dataset\")\n",
    "    plt.show()\n",
    "\n",
    "# Boxplots\n",
    "# Shows the distribution of the target variable within each category of the categorical feature.\n",
    "def visualize_boxplots(df, target_variable, categorical_feature):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=categorical_feature, y=target_variable, data=df)\n",
    "    plt.title(\"Boxplot of Target Variable Across Categories\")\n",
    "    plt.show()\n",
    "\n",
    "# Pairwise Feature Scatter Plots\n",
    "def visualize_pairwise_scatter_plots(df):\n",
    "    sns.pairplot(df.sample(frac=0.1, random_state=42))\n",
    "    plt.title(\"Pairwise Scatter Plots of Features\")\n",
    "    plt.show()\n",
    "\n",
    "# Feature Importance Plot\n",
    "def visualize_feature_importance(model, feature_names):\n",
    "    importances = None\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_)\n",
    "    \n",
    "    if importances is not None and len(importances) > 0:\n",
    "        # Create a DataFrame to sort and display feature importances\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        })\n",
    "\n",
    "        # Sort the features by importance\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        # Plot the feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title(\"Feature Importance Plot\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Geospatial Visualization (if applicable)\n",
    "# Time Series Plots (if applicable)\n",
    "# Interactive Plots (if applicable)\n",
    "\n",
    "# 1.2 Use the above subfunctions to plot for dataset\n",
    "# Load dataset\n",
    "try:\n",
    "    dataset_name = 'boston'\n",
    "    data = load_dataset(dataset_name)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Select numeric columns only\n",
    "df = data.select_dtypes(include=['float64', 'int64'])\n",
    "target_variable='mpg'\n",
    "df_without_target=df.drop(columns=target_variable)\n",
    "\n",
    "plot_all_data(df,df_without_target,target_variable,model, feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "def fit_regression_models(X,y,testsize,degree_list=None, num_bootstrap_samples=50, confidence = 0.95, random_seed=42,top_importances=10, model_step_name=\"regression_model\"):\n",
    "    # 3. Split the Data\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize, random_state=random_seed)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while splitting the data: {e}\")\n",
    "        exit()\n",
    "    column_names=X_train.columns.tolist()\n",
    "    target_variable=y_train.name\n",
    "    # 4. Define Regression Models\n",
    "    regression_models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(),\n",
    "        'Lasso Regression': Lasso(),\n",
    "        'ElasticNet Regression': ElasticNet(),\n",
    "        'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "        'Random Forest Regressor': RandomForestRegressor(),\n",
    "        'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
    "        'Support Vector Regressor': SVR(),\n",
    "        'K-Neighbors Regressor': KNeighborsRegressor(),\n",
    "        'Neural Network Regressor': MLPRegressor(max_iter=1000)  # Increased max_iter to ensure convergence\n",
    "    }\n",
    "    model_params = {\n",
    "        'Random Forest Regressor': {\n",
    "            'n_estimators': [50, 100, 200], \n",
    "            'max_features': ['auto', 'sqrt', 'log2'], \n",
    "            'max_depth': [None, 10, 20, 30], \n",
    "            'min_samples_split': [2, 5, 10], \n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Gradient Boosting Regressor': {\n",
    "            'n_estimators': [50, 100, 200], \n",
    "            'learning_rate': [0.01, 0.1, 0.5, 1.0], \n",
    "            'max_depth': [3, 4, 5], \n",
    "            'min_samples_split': [2, 5, 10], \n",
    "            'min_samples_leaf': [1, 2, 4], \n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "   \n",
    "    # Add polynomial regression models with each degree in the degree_list\n",
    "    if degree_list is None:\n",
    "        degree_list=[1] # Default to linear if no degrees are specified\n",
    "    else: # add 1 into the list for later loop\n",
    "        degree_list = list(degree_list)\n",
    "        if 1 not in degree_list:\n",
    "            degree_list.append(1)\n",
    "    # After appending, the degree_list wont be empty\n",
    "\n",
    "    # Function to create a pipeline with poly features\n",
    "    # When degree>1 apply pipelines to those models which support poly\n",
    "    def create_pipeline(model, degree):\n",
    "        if degree > 1:\n",
    "            return Pipeline([\n",
    "                (\"poly_features\", PolynomialFeatures(degree=degree)),\n",
    "                (\"regression_model\", model)\n",
    "            ])\n",
    "        else:\n",
    "            return Pipeline([\n",
    "                (\"regression_model\", model)\n",
    "            ])\n",
    "\n",
    "    regression_pipelines = {}\n",
    "    for name, model in regression_models.items():    \n",
    "        # Add all models into the pipelines\n",
    "        for degree in degree_list:\n",
    "            regression_pipelines[f'{name} (Degree {degree})'] = Pipeline([\n",
    "                (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "                (f\"{model_step_name}\", GridSearchCV(model, model_params[name], cv=5))\n",
    "            ])\n",
    "         \n",
    "    # print(regression_pipelines)\n",
    "    best_model_info = None\n",
    "    best_model = None\n",
    "    metrics_list = []\n",
    "\n",
    "    for name, pipeline in regression_pipelines.items():\n",
    "        try: \n",
    "            # 5. Train the model\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Save the pipelines to disk\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            dump(pipeline, f\"{name}.joblib\")\n",
    "            \n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            residuals = y_test - y_pred\n",
    "\n",
    "            # Calculate variance and bias using cross-validation\n",
    "            variance=np.var(y_pred)\n",
    "            bias=np.mean(np.abs(y_test-y_pred))\n",
    "\n",
    "            # Estimate model complexity (number of parameters)\n",
    "            if hasattr(model, 'coef_'):\n",
    "                complexity = len(model.coef_)\n",
    "            elif hasattr(model, 'n_estimators'):\n",
    "                complexity = model.n_estimators\n",
    "            elif hasattr(model, 'alpha'):\n",
    "                complexity = model.alpha\n",
    "            else:\n",
    "                complexity = None\n",
    "\n",
    "            # Determine the degree for regression models\n",
    "            import re\n",
    "            match = re.search(r'Degree (\\d+)', name)\n",
    "            degree = int(match.group(1))\n",
    "\n",
    "            # Store results for comparison\n",
    "            metrics_list.append({\n",
    "                'model': name,\n",
    "                \"pipeline\":pipeline,\n",
    "                'degree': degree, \n",
    "                'r2': r2,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'residuals_mean': residuals.mean(),\n",
    "                'residuals_std': residuals.std(),\n",
    "                'variance': variance,\n",
    "                'bias': bias,\n",
    "                'complexity': complexity\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while training and evaluating the {name} model: {e}\")\n",
    "\n",
    "    # Sort the metrics by R² in descending order and other metrics in ascending order\n",
    "    metrics_list.sort(key=lambda x: (-x['r2'], x['mse'], x['rmse'], x['mae'], x['residuals_mean'],x['residuals_std'], x['variance'],x['bias'],x['complexity']))\n",
    "    # df_metrics=pd.DataFrame(metrics_list)\n",
    "    # print(df_metrics)\n",
    "    print(metrics_list)\n",
    "\n",
    "    # 6. Return the best model\n",
    "    best_model_info=metrics_list[0]\n",
    "    best_model_name =best_model_info['model']\n",
    "    best_model = best_model_info['pipeline'] \n",
    "    degree = best_model_info['degree']\n",
    "    print(f'Best model found: {best_model_name}')\n",
    "    # print(best_model_info)\n",
    "\n",
    "    # Load the pipe line before predict\n",
    "    # best_model=load(f\"{best_model_name}.joblib\")\n",
    "\n",
    "    # Delete all other joblib files\n",
    "    for name, pipeline in regression_pipelines.items():\n",
    "        if name != best_model_name:\n",
    "            joblib_file = f'{name}.joblib'\n",
    "            if os.path.exists(joblib_file):\n",
    "                os.remove(joblib_file)\n",
    "    \n",
    "    # 6.1 (Optional) Calculate the prediction_intervals based on the best_model\n",
    "    # Resample on the train data to get sample data setS using bootstrap \n",
    "    # keep fitting with the same model type and generating new model each time\n",
    "    # calculate prediction intervals\n",
    "    bootstrap_predictions =[]\n",
    "    prediction_intervals = []\n",
    "\n",
    "    # In scikit-learn, the fit method is used to train a model, \n",
    "    # but it does not return a result object like in statsmodels. \n",
    "    # Instead, the model instance itself is updated with the learned parameters. \n",
    "    # One way to train the same model class is to clone the model type of the best_model  \n",
    "    # on multiple sample datasets\n",
    "    \n",
    "    # We do not need to manually fit and transform X_train if we use a pipeline. \n",
    "    # The Pipeline object in scikit-learn takes care of applying the fit and transform methods for each step in the pipeline. \n",
    "    # When we call fit on the pipeline, it will sequentially call fit and transform for each step, passing the transformed data to the next step. Similarly, when you call predict or transform on the pipeline, \n",
    "    # it will apply the transformations and predictions in the order specified.\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "\n",
    "        indices = np.random.choice(np.arange(0, len(X_train)), len(X_train), replace=True)\n",
    "\n",
    "        # Get one set of bootstrap sample data\n",
    "        X_bootstrap= X_train.iloc[indices]\n",
    "        y_bootstrap= y_train.iloc[indices]\n",
    "        \n",
    "        # Create a new model instance of the same type as 'best_model'\n",
    "        bootstrap_model = clone(best_model)\n",
    "        # Fit the new model to the bootstrapped sample\n",
    "        bootstrap_model.fit(X_bootstrap, y_bootstrap)\n",
    "        # Make predictions on the test data using the new model\n",
    "        predictions_sample = bootstrap_model.predict(X_test)\n",
    "        # Append predictions to the list\n",
    "        bootstrap_predictions.append(predictions_sample)\n",
    "\n",
    "    # Compute prediction intervals \n",
    "    lower_bound = np.percentile(bootstrap_predictions, (1 - confidence) * 100 / 2, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_predictions, (1 + confidence) * 100 / 2, axis=0)\n",
    "    prediction_intervals = np.column_stack((lower_bound, upper_bound))\n",
    "    # print(prediction_intervals)\n",
    "    \n",
    "    # 6.2  Get the top important features\n",
    "    # 1) Extract the feature names from the polynomial features\n",
    "    poly_features = best_model.named_steps['poly_features']\n",
    "    column_names = X_train.columns.tolist() \n",
    "    feature_names = poly_features.get_feature_names_out(input_features=column_names)\n",
    "    \n",
    "    # Below can be omitted when include_bias=False, feature_names won't contain const\n",
    "    # Remove 'x0', 'x1' prefix from feature names\n",
    "    feature_names = [name.replace('x0', 'x1').replace('x1', 'x2') for name in feature_names]\n",
    "\n",
    "    # 2) Get feature importances or coefficients\n",
    "    importances = None\n",
    "    model=best_model.named_steps[f'{model_step_name}']\n",
    "    print(f\"Getting feature importances from model: '{model}'\")\n",
    "    # In scikit-learn, the feature_importances_ attribute is typically used for tree-based models \n",
    "    # like Random Forests, Gradient Boosting Machines, etc. \n",
    "    # These models do not have a concept of a constant term or intercept like linear models do. \n",
    "    # Therefore, we won't find a constant term in the feature importances.\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_)\n",
    "    elif hasattr(model, 'dual_coef_'):  # Check for dual coefficients\n",
    "        importances = np.abs(model.dual_coef_)\n",
    "    elif hasattr(model, 'params'):  # Assuming 'params' contains the coefficients\n",
    "        importances = np.abs(model.params)  \n",
    "    elif hasattr(model, 'tvalues'):\n",
    "        importances = np.abs(model.tvalues)\n",
    "    else:\n",
    "        print(\"No feature importances available. Plotting for all features...\")\n",
    "    \n",
    "    top_features = None\n",
    "    if len(importances) > 0:\n",
    "        \n",
    "        num_original_features = X_train.shape[1]\n",
    "        print(f\"Originally, X_train contains {num_original_features} columns.\")\n",
    "        print(f\"after fitting the model, features become {len(feature_names)},\")\n",
    "        print(f\" and the feature importances contain {len(importances)} values.\")\n",
    "  \n",
    "        if len(importances) == len(feature_names):\n",
    "            # Reshape importances array if necessary\n",
    "            importances = np.ravel(importances)\n",
    "\n",
    "        else:\n",
    "            # Calculate the interaction terms from the feature_names \n",
    "            # Importances are just importances for the original columns from X_train\n",
    "            importances_original = importances\n",
    "            importances_interaction = []\n",
    "\n",
    "            for name in feature_names[num_original_features:]:\n",
    "                # Split the term into components if there are spaces (' ')\n",
    "                components = name.split(' ')\n",
    "                importance_interaction=1\n",
    "                for component in components:\n",
    "                    if '^' in component:\n",
    "                        base, power =component.split('^')\n",
    "                    else: \n",
    "                        base =component\n",
    "                        power =1 \n",
    "\n",
    "                    base_index= feature_names.index(base)\n",
    "                \n",
    "                    importance_component = importances_original[base_index] ** int(power)\n",
    "                    \n",
    "                    importance_interaction *= importance_component\n",
    "                # Append the importance of the interaction term to the list\n",
    "                importances_interaction.append(importance_interaction)\n",
    "\n",
    "            # Combine the importances for original columns and interaction terms\n",
    "            importances = np.concatenate((importances_original, importances_interaction))\n",
    "\n",
    "\n",
    "        # Create a DataFrame to sort and display feature importances\n",
    "\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        })\n",
    "        # Sort the features by importance\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        # Get the top N important features\n",
    "        top_features = feature_importance_df['feature'].head(top_importances).values\n",
    "    else: \n",
    "        top_features=feature_names\n",
    "    print(f\"top_features: {top_features}\")\n",
    "\n",
    "    # 6.3 Get the full equation from the pipeline\n",
    "    # As 6.2 does not involve any const / intercept due to include_bias =False:\n",
    "    # Note: For models in scikit-learn that do not have a built-in intercept_ attribute, such as tree-based models \n",
    "    # (e.g., DecisionTreeRegressor, RandomForestRegressor) or support vector machines (SVM), \n",
    "    # there isn't a direct attribute that represents the intercept value like in linear models.\n",
    "    # Only get intercept for linear regression model\n",
    "    intercept =0\n",
    "    try:\n",
    "        intercept = model.intercept_\n",
    "    except:\n",
    "        print(\"No intercept for this model, as include_bias is set to be False.\")\n",
    "    # Construct the full equation\n",
    "    full_equation = f\"{target_variable} = {intercept:.2f}\"\n",
    "    for feature_name, importance in zip(feature_importance_df['feature'], feature_importance_df['importance']):\n",
    "        full_equation += f\" + {importance:.2f} * {feature_name}\"\n",
    "    print(full_equation)\n",
    "    \n",
    "    return best_model_name, degree , top_features, prediction_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "# Subfunctions to visualize performance metrics\n",
    "def plot_all_metrics(best_model_name,  X,y, testsize, top_features, degree, prediction_intervals,model_step_name, random_seed=42, param_name=None, param_range=None):\n",
    "    # 1. Split the data using the same random_seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize, random_state=random_seed)\n",
    "    \n",
    "    # Define metrics to plot\n",
    "    metrics_to_plot = [\n",
    "        (plot_predicted_vs_actual, 'Predicted vs. Actual Values'),\n",
    "        (plot_learning_curve, 'Learning Curve'),\n",
    "        (plot_residual_plot, 'Residual Plot'),\n",
    "        (plot_studentized_residuals, 'Studentized Residuals vs. Fitted Values'),\n",
    "        (plot_standardized_residuals, 'Standardized Residuals'),\n",
    "        (plot_validation_curve, 'Validation Curve'),\n",
    "        (plot_poly_degree_vs_mse, 'Poly Degree vs. mse'),\n",
    "        (plot_bias_variance_mse,'Poly Test bias Variance'),\n",
    "        (plot_prediction_intervals, 'Prediction Intervals'),\n",
    "        (plot_prediction_actual, 'Predicted vs. Actual, Scatter')\n",
    "        \n",
    "    ]\n",
    "\n",
    "    # Below are not appropriate for Regression models:\n",
    "    # Confusion Matrix \n",
    "    # ROC Curve \n",
    "    # Precision-Recall Curve\n",
    "\n",
    "    # Below are better based on train data rather than test data:\n",
    "    # Learning Curve,\n",
    "    # Validation Curve,\n",
    "    # Bias-Variance\n",
    "    # Prediction Intervals\n",
    "    # Studentized Residuals\n",
    "    # Standardized Residuals\n",
    "\n",
    "    # 2. Calculate y_pred, residuals for plotting\n",
    "    # Before prediction, we need to load the pipeline\n",
    "    pipeline = load(f'{best_model_name}.joblib')\n",
    "\n",
    "    # 3. Create the figure and axes\n",
    "    num_metrics = len(metrics_to_plot)\n",
    "    num_cols = 3\n",
    "    num_rows = (num_metrics + num_cols - 1) // num_cols\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(6.4*num_cols, 4.8*num_rows))\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=1, hspace=1) \n",
    "\n",
    "    # Flatten the axes array\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop over the metrics and titles, plotting each one\n",
    "    for i, (plot_function, title) in enumerate(metrics_to_plot):\n",
    "        # Plot validation curve only if degree > 0\n",
    "        if title == 'Validation Curve':\n",
    "            if degree >0:\n",
    "                param_name = 'poly_features__degree'\n",
    "                param_range = np.arange(1,6)\n",
    "                plot_function(pipeline, X_train, y_train, param_name, param_range, ax=axes[i])\n",
    "        elif title == 'Poly Degree vs. mse' or title =='Poly Test bias Variance':\n",
    "            if degree >0: \n",
    "                param_range = np.arange(1,6)\n",
    "                plot_function(pipeline, X_train, y_train, X_test, y_test, param_range, model_step_name, ax=axes[i])\n",
    "        # Plot other metrics directly\n",
    "        elif 'Learning' in title:\n",
    "            plot_function(pipeline, X_train, y_train, ax=axes[i], cv=5)\n",
    "        elif title == \"Prediction Intervals\":\n",
    "            plot_function(pipeline, X_train, y_train, X_test, y_test, prediction_intervals, best_model_name, ax= axes[i])\n",
    "        elif title in (\"Standardized Residuals\", \"Studentized Residuals vs. Fitted Values\"):\n",
    "            plot_function(pipeline, X_train, y_train, ax=axes[i])\n",
    "  \n",
    "        else:\n",
    "            plot_function(pipeline, X_test, y_test, best_model_name, ax=axes[i] )\n",
    "\n",
    "    # Hide any empty grid cells\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predicted_vs_actual(pipeline, X_test, y_test, best_model_name, ax):\n",
    "    \"\"\"\n",
    "    Plot the predicted vs. actual values.\n",
    "\n",
    "    Parameters:\n",
    "    model (object): The trained regression model.\n",
    "    X_test (array-like): The test data features.\n",
    "    y_test (array-like): The actual target values for the test data.\n",
    "    ax (matplotlib axes): The axes to plot the figure.\n",
    "    \"\"\"\n",
    "    y_pred=pipeline.predict(X_test)\n",
    "\n",
    "    # Below is to give the plot a 45 degree line\n",
    "    # Define the range for the plot\n",
    "    min_val = min(min(y_test), min(y_pred))\n",
    "    max_val = max(max(y_test), max(y_pred))\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "\n",
    "    ax.scatter(y_test, y_pred)\n",
    "    ax.set_xlabel(\"Actual y Values\")\n",
    "    ax.set_ylabel(\"Predicted y Values\")\n",
    "    ax.set_title(\"Predicted vs. Actual Values\\nsk-learn\") \n",
    "\n",
    "def plot_learning_curve(pipeline, X_train, y_train, ax, cv=None):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(pipeline, X_train, y_train, cv=cv, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    ax.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training mse')\n",
    "    ax.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "    ax.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='Validation mse')\n",
    "    ax.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('Number of training samples')\n",
    "    ax.set_ylabel('Mean Square Error')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title('Learning Curve\\nsk-learn')\n",
    "\n",
    "def plot_residual_plot(pipeline, X_test, y_test, best_model_name, ax):\n",
    "    y_pred=pipeline.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "    ax.scatter(y_pred, residuals)\n",
    "    ax.axhline(y=0, color='red', linestyle='--')\n",
    "    ax.set_title('Residual vs. Fitted Values\\nsk-learn')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "\n",
    "    # in Statsmodels library, we can use below to create plots directly:\n",
    "    # fig,ax = plt.subplots(1,1, figsize=(10, 6.18))\n",
    "    # sm.graphics.plot_regress_exog(results, 'horsepower', fig=fig)\n",
    "    # plt.title('Residuals vs. Fitted Values')\n",
    "\n",
    "# Before we plot for studentized_residuals or standardized_residuals\n",
    "# We need to make sure these residuals are calculated based on train data or test data\n",
    "# Based on train data, the residuals will be more stable especially when test data is in a small amount\n",
    "# Based on test data, the residuals may not capture the variability of the entire dataset??\n",
    "# Finally, I decided to use train data for these 2 plots\n",
    "# If the analysis focuses on new data predictions, change X_train --> X_test\n",
    "\n",
    "def plot_studentized_residuals(pipeline, X_train, y_train, ax):\n",
    " \n",
    "    y_pred_train=pipeline.predict(X_train)\n",
    "    residuals_train = y_train - y_pred_train\n",
    "\n",
    "    # Calculate studentized residuals\n",
    "    # Hat matrix\n",
    "    hat_matrix = np.dot(X_train, np.linalg.inv(np.dot(X_train.T, X_train))).dot(X_train.T)\n",
    "    leverage = np.diag(hat_matrix)\n",
    "    \n",
    "    # Calculate the studentized residuals\n",
    "    std_residuals = np.std(residuals_train)\n",
    "    studentized_residuals = residuals_train / (std_residuals * np.sqrt(1 - leverage))\n",
    "\n",
    "    ax.scatter(y_pred_train, studentized_residuals, alpha=0.8)\n",
    "    ax.axhline(0, color='blue', linestyle='--')\n",
    "    ax.set_xlabel('Fitted Values')\n",
    "    ax.set_ylabel('Studentized Residuals')\n",
    "    ax.set_title('Studentized Residuals vs. Fitted Values\\nsk-learn')\n",
    "    \n",
    "\n",
    "def plot_standardized_residuals(pipeline, X_train, y_train, ax):\n",
    "    # Scale Location Plot\n",
    "    # plot the square root of the standardized residuals against the fitted values \n",
    "    # to check for homoscedasticity.\n",
    "    # Below .fittedvalues and .resid is from sm lib\n",
    "    # y_pred = model.fittedvalues\n",
    "    # residuals = model.resid\n",
    "    # Below is to calculate y_pred and residuals using sk-learn lib\n",
    "    y_pred_train= pipeline.predict(X_train)\n",
    "    residuals_train = y_train - y_pred_train\n",
    "    std_residuals = np.std(residuals_train)\n",
    "    # Standardize residuals\n",
    "    standardized_residuals = residuals_train / std_residuals\n",
    "    \n",
    "    sqrt_standardized_residuals = np.sqrt(np.abs(standardized_residuals))\n",
    "    ax.axhline(np.mean(sqrt_standardized_residuals), color='red', linestyle='-')\n",
    "    ax.scatter(x=y_pred_train, y=sqrt_standardized_residuals)\n",
    "    ax.axhline(0, color='blue', linestyle='--')\n",
    "    ax.set_xlabel('Fitted Values')\n",
    "    ax.set_ylabel('Sqrt(|Standardized residuals|)')\n",
    "    ax.set_title('Scale-Location Plot\\nsk-learn')\n",
    "\n",
    "\n",
    "def plot_validation_curve(pipeline, X_train, y_train, param_name, param_range, ax):\n",
    "    \"\"\"\n",
    "    Plot validation curve to visualize the effect of model flexibility on performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    model (object): The regression model.\n",
    "    X_train (array-like): The training data features.\n",
    "    y_train (array-like): The target values for the training data.\n",
    "    param_name (str): The name of the hyperparameter to vary.\n",
    "    param_range (array-like): The range of values for the hyperparameter.\n",
    "    ax (matplotlib.axes.Axes, optional): The axes to plot the validation curve. If not provided, a new figure and axes will be created.\n",
    "\n",
    "    Returns:\n",
    "    ax (matplotlib.axes.Axes): The axes containing the validation curve plot.\n",
    "    \"\"\"\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        pipeline, X_train, y_train, param_name=param_name, param_range=param_range,\n",
    "        scoring=\"neg_mean_squared_error\", cv=5, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    train_mean = -np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = -np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(param_range, train_mean, color=\"blue\", marker=\"o\", markersize=5, label=\"Training MSE\")\n",
    "    ax.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.15, color=\"blue\")\n",
    "    ax.plot(param_range, test_mean, color=\"green\", linestyle=\"--\", marker=\"s\", markersize=5, label=\"Validation MSE\")\n",
    "    ax.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.15, color=\"green\")\n",
    "\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel(\"Mean Squared Error\")\n",
    "    ax.set_title(\"Validation Curve\\nsk-learn\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "# Define a function to create the pipeline with a given degree\n",
    "def create_pipeline(degree, regressor):\n",
    "    return make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        regressor\n",
    "    )\n",
    "def plot_poly_degree_vs_mse(pipeline, X_train, y_train, X_test, y_test, param_range, model_step_name, ax):\n",
    "    \n",
    "    train_mse = []\n",
    "    test_mse = []\n",
    "    regressor = pipeline.named_steps[f\"{model_step_name}\"]\n",
    "    for degree in param_range:\n",
    "        \n",
    "        new_pipeline=create_pipeline(degree, regressor)\n",
    "        # Fit the model\n",
    "        new_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = new_pipeline.predict(X_train)\n",
    "        y_test_pred = new_pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate MSE for training and test sets\n",
    "        train_mse.append(mean_squared_error(y_train, y_train_pred))\n",
    "        test_mse.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    # Plotting\n",
    "    ax.plot(param_range, train_mse, label='Training MSE')\n",
    "    ax.plot(param_range, test_mse, label='Test MSE')\n",
    "    ax.set_xlabel('Degree of Polynomial')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_title('Training and Test MSE vs. Polynomial Degree\\nsk-learn')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "def calculate_bias_variance(pipeline, X_train, y_train, X_test, y_test):\n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Bias and variance calculation\n",
    "    bias = np.mean((y_test - (y_test_pred)) ** 2)\n",
    "    variance = np.mean((y_test_pred - np.mean(y_test_pred)) ** 2)\n",
    "    \n",
    "    # Test MSE calculation\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    return test_mse, bias, variance\n",
    "\n",
    "def plot_bias_variance_mse(pipeline, X_train, y_train, X_test, y_test, degree_range, model_step_name, ax):\n",
    "    test_mse_list = []\n",
    "    bias_list = []\n",
    "    variance_list = []\n",
    "    regressor = pipeline.named_steps[f'{model_step_name}']\n",
    "    for degree in degree_range:\n",
    "        # Create polynomial features and linear regression model\n",
    "        new_pipeline = create_pipeline(degree, regressor)\n",
    "        \n",
    "        # Calculate test MSE, bias, and variance\n",
    "        test_mse, bias, variance = calculate_bias_variance(new_pipeline, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Append to lists\n",
    "        test_mse_list.append(test_mse)\n",
    "        bias_list.append(bias)\n",
    "        variance_list.append(variance)\n",
    "\n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(degree_range, test_mse_list, label='Test MSE', marker='o',linewidth=5)\n",
    "    ax.plot(degree_range, bias_list, label='Bias^2', marker='o')\n",
    "    ax.plot(degree_range, variance_list, label='Variance', marker='o')\n",
    "    ax.set_xlabel('Degree of Polynomial Features')\n",
    "    ax.set_ylabel('Mean Square Error')\n",
    "    ax.set_title('Bias-Variance Tradeoff\\nsk-learn')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "def plot_prediction_intervals(pipeline, X_train, y_train, X_test, y_test, prediction_intervals, best_model_name, ax):\n",
    "    # Prediction intervals provide a range of values within \n",
    "    # which future observations are expected to fall, with a certain level of confidence. \n",
    "    # Bootstrapping is a resampling method that can be used to estimate the uncertainty of a statistical estimate, including prediction intervals. \n",
    "    # It involves repeatedly resampling the data with replacement \n",
    "    # and fitting the model to each resampled dataset to generate a distribution of predictions.\n",
    "    pipeline = load(f'{best_model_name}.joblib')\n",
    "    # Generate predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Plot the 'actual' predicted values\n",
    "    ax.scatter(np.arange(len(X_test)), y_pred, label='Predictions')\n",
    "    \n",
    "    # Plot the prediction intervals\n",
    "    ax.fill_between(np.arange(len(X_test)), prediction_intervals[:, 0], prediction_intervals[:, 1], alpha=0.3, color='orange', label=f'{int(confidence * 100)}% Prediction Intervals')\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Sample Index') # index for each data point , or index for each row\n",
    "    ax.set_ylabel('Predicted Value')\n",
    "    ax.set_title('Prediction Intervals\\nsk-learn')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_prediction_actual(pipeline, X_test, y_test,best_model_name, ax):\n",
    "    pipeline = load(f'{best_model_name}.joblib')\n",
    "     # Generate predictions for the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Plot the actual values\n",
    "    ax.scatter(np.arange(len(X_test)), y_test, label='Actual Values')\n",
    "    \n",
    "    # Plot the actual values\n",
    "    ax.scatter(np.arange(len(X_test)), y_pred, label='Predicted Values')\n",
    "\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Data Points') \n",
    "    ax.set_ylabel('Predicted Value')\n",
    "    ax.set_title('Prediction vs. Actual\\nsk-learn')\n",
    "    ax.legend()\n",
    "    # del pipeline # to free up the memory\n",
    "# Subfunction to visualize dataset according to data characteristics\n",
    "def visualize_dataset(df):\n",
    "    print(\"Starting to visualize dataset...\")\n",
    "    sns.pairplot(df.sample(frac=0.1, random_state=42), hue='target', palette='bwr')\n",
    "    plt.title(\"Pairplot of Dataset\")\n",
    "    plt.show()\n",
    "\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "    df.describe().transpose().plot(kind='hist', bins=20, subplots=True, layout=(5, 6), figsize=(20, 15))\n",
    "    plt.suptitle(\"Distributions of Features\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Subfunctions to load the dataset\n",
    "def find_filepath(name):\n",
    "    for root, dirs, files in os.walk('..'):\n",
    "        for file in files:\n",
    "            base, extension = os.path.splitext(file)\n",
    "            if extension.lower() in ('.txt', '.csv'):\n",
    "                if os.path.splitext(name)[1]:  # If name has an extension\n",
    "                    if file.lower() == name.lower():  # Compare filename with name\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        if os.path.isfile(file_path):\n",
    "                            return file_path\n",
    "                else:  # If name does not have an extension\n",
    "                    if base == os.path.splitext(name)[0]:  # Compare base part of filename with name\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        if os.path.isfile(file_path):\n",
    "                            return file_path\n",
    "    return None\n",
    "\n",
    "def get_dataframe(original_data, target_variable=None):\n",
    "    if isinstance(original_data, pd.DataFrame):\n",
    "        return original_data\n",
    "    else:\n",
    "        try:\n",
    "            data = pd.DataFrame(original_data.data, columns=original_data.feature_names)\n",
    "            data[target_variable] = original_data.target\n",
    "            return data\n",
    "        except:\n",
    "            data = pd.DataFrame(original_data)\n",
    "            return data\n",
    "\n",
    "def capitalize_first_letter(word):\n",
    "    # Capitalize the first letter and convert the rest of the word to lowercase\n",
    "    return word[:1].upper() + word[1:].lower()\n",
    "\n",
    "def load_dataset(name):\n",
    "    try: \n",
    "        \n",
    "        capitalized_name = capitalize_first_letter(name)\n",
    "        target_variable = None\n",
    "        data = load_data(f\"{capitalized_name}\")\n",
    "        \n",
    "        return get_dataframe(data, target_variable)\n",
    "    except:\n",
    "        if name == 'california_housing':\n",
    "            target_variable = 'MedHouseVal'\n",
    "            original_data = fetch_california_housing()\n",
    "            return get_dataframe(original_data, target_variable), target_variable\n",
    "        else: \n",
    "            return 'No dataset named ' + capitalized_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 2. Load the Dataset (Boston dataset)\n",
    "    try:\n",
    "        dataset_name = 'boston'\n",
    "        # dataset_name = \"auto\"\n",
    "        data = load_dataset(dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Define X and Y\n",
    "    drop_column_list = [target_variable]\n",
    "    y= data[target_variable]\n",
    "    X= data.drop(columns= drop_column_list)\n",
    "\n",
    "    # Specify the degree of the polynomial and test size\n",
    "    degree_list = [1,2]  # Replace with desired degree or obtain from user input\n",
    "    testsize = 0.2  # Replace with desired test size or obtain from user input\n",
    "    \n",
    "    # 4. Train and Evaluate Regression Models and get the best model\n",
    "    confidence = 0.95\n",
    "    random_seed=42\n",
    "    num_bootstrap_samples=1\n",
    "    top_importances = 10\n",
    "    model_step_name=\"regression_model\"\n",
    "    best_model_name, degree ,top_features, prediction_intervals=fit_regression_models(X,y,testsize,degree_list, num_bootstrap_samples, confidence, random_seed,top_importances,model_step_name)\n",
    "\n",
    "    # 5. Visualize Model Performance \n",
    "    plot_all_metrics(best_model_name, X,y, testsize, top_features, degree, prediction_intervals, model_step_name, random_seed)\n",
    "\n",
    "    # 6. Predictions on new data\n",
    "    # Predict single observation:\n",
    "    if best_model is not None:\n",
    "        sample_data = X_test.iloc[[0]]  # Example: Use the first instance from the test set\n",
    "        prediction = best_model.predict(sample_data)\n",
    "        print(f\"Prediction for sample data: {prediction}\")\n",
    "    else:\n",
    "        print(\"No best regression model found, cannot make predictions.\")\n",
    "###### scikit-learn/regression ######"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
